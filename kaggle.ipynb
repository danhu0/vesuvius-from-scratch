{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/vesuvius-challenge/train/1/ir.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Viz\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPREFIX\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mir.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vesuviusgp/lib/python3.12/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/vesuvius-challenge/train/1/ir.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "PREFIX = 'train/1/'\n",
    "BUFFER = 30  # Buffer size in x and y direction\n",
    "Z_START = 27 # First slice in the z direction to use\n",
    "Z_DIM = 10   # Number of slices in the z direction\n",
    "TRAINING_STEPS = 30000\n",
    "LEARNING_RATE = 0.03\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Viz\n",
    "plt.imshow(Image.open(PREFIX+\"ir.png\"), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load these binary images:\n",
    "\n",
    " - __mask.png__: a mask of which pixels contain data, and which pixels we should ignore.\n",
    " - __inklabels.png__: our label data: whether a pixel contains ink or no ink (which has been hand-labeled based on the infrared photo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(PREFIX+\"mask.png\").convert('1')) # .convert('1') converts to black&white\n",
    "label = torch.from_numpy(np.array(Image.open(PREFIX+\"inklabels.png\"))).gt(0).float().to(DEVICE) # .gt(0) converts to binary mask\n",
    "\n",
    "# Viz\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title(\"mask.png\")\n",
    "ax1.imshow(mask, cmap='gray')\n",
    "ax2.set_title(\"inklabels.png\")\n",
    "ax2.imshow(label.cpu(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the 3d x-ray of the fragment. This is represented as a .tif image stack. The image stack is an array of 16-bit grayscale images. Each image represents a \"slice\" in the z-direction, going from below the papyrus, to above the papyrus. We'll convert it to a 4D tensor of 32-bit floats. We'll also convert the pixel values to the range [0, 1].\n",
    "\n",
    "To save memory, we'll only load the innermost slices (Z_DIM of them). Let's look at them when we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 3d x-ray scan, one slice at a time\n",
    "images = [np.array(Image.open(filename), dtype=np.float32)/65535.0 for filename in tqdm(sorted(glob.glob(PREFIX+\"surface_volume/*.tif\"))[Z_START:Z_START+Z_DIM])]\n",
    "image_stack = torch.stack([torch.from_numpy(image) for image in images], dim=0).to(DEVICE)\n",
    "\n",
    "# Viz\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
    "for image, ax in zip(images, axes):\n",
    "  ax.imshow(np.array(Image.fromarray(image).resize((image.shape[1]//20, image.shape[0]//20)), dtype=np.float32), cmap='gray')\n",
    "  ax.set_xticks([]); ax.set_yticks([])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the ink in these slices of the 3d x-ray scan..? Neither can we.\n",
    "\n",
    "Now we'll create a dataset of subvolumes. We use a small rectangle around the letter \"P\" for our evaluation, and we'll exclude those pixels from the training set. (It's actually a Greek letter \"rho\", which looks similar to our \"P\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = (1100, 3500, 700, 950)\n",
    "\n",
    "# Viz\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(label.cpu())\n",
    "patch = patches.Rectangle((rect[0], rect[1]), rect[2], rect[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(patch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a PyTorch dataset and (super simple) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch needs these custom dataset classes for the model with len and getitem defined\n",
    "class SubvolumeDataset(data.Dataset):\n",
    "    def __init__(self, image_stack, label, pixels):\n",
    "        self.image_stack = image_stack\n",
    "        self.label = label\n",
    "        self.pixels = pixels\n",
    "    def __len__(self):\n",
    "        return len(self.pixels)\n",
    "    # Return data and label at given index\n",
    "    def __getitem__(self, index):\n",
    "        y, x = self.pixels[index]\n",
    "        subvolume = self.image_stack[:, y-BUFFER:y+BUFFER+1, x-BUFFER:x+BUFFER+1].view(1, Z_DIM, BUFFER*2+1, BUFFER*2+1) # .view = .reshape\n",
    "        inklabel = self.label[y, x].view(1)\n",
    "        return subvolume, inklabel\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv3d(1, 16, 3, 1, 1), nn.MaxPool3d(2, 2),\n",
    "    nn.Conv3d(16, 32, 3, 1, 1), nn.MaxPool3d(2, 2),\n",
    "    nn.Conv3d(32, 64, 3, 1, 1), nn.MaxPool3d(2, 2),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.LazyLinear(128), nn.ReLU(),\n",
    "    nn.LazyLinear(1), nn.Sigmoid()\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll train the model. Typically it takes about 10 minutes.\n",
    "\n",
    "print(\"Generating pixel lists...\")\n",
    "# Split our dataset into train and val. The pixels inside the rect are the \n",
    "# val set, and the pixels outside the rect are the train set.\n",
    "# Adapted from https://www.kaggle.com/code/jamesdavey/100x-faster-pixel-coordinate-generator-1s-runtime\n",
    "# Create a Boolean array of the same shape as the bitmask, initially all True\n",
    "not_border = np.zeros(mask.shape, dtype=bool)\n",
    "not_border[BUFFER:mask.shape[0]-BUFFER, BUFFER:mask.shape[1]-BUFFER] = True\n",
    "arr_mask = np.array(mask) * not_border  # new mask in np.array form excluding the very edges (size buffer padding)\n",
    "inside_rect = np.zeros(mask.shape, dtype=bool) * arr_mask\n",
    "# Sets all indexes with inside_rect array to True\n",
    "inside_rect[rect[1]:rect[1]+rect[3]+1, rect[0]:rect[0]+rect[2]+1] = True\n",
    "# Set the pixels within the inside_rect to False\n",
    "outside_rect = np.ones(mask.shape, dtype=bool) * arr_mask\n",
    "outside_rect[rect[1]:rect[1]+rect[3]+1, rect[0]:rect[0]+rect[2]+1] = False\n",
    "pixels_inside_rect = np.argwhere(inside_rect)\n",
    "pixels_outside_rect = np.argwhere(outside_rect)\n",
    "\n",
    "print(\"Training...\")\n",
    "train_dataset = SubvolumeDataset(image_stack, label, pixels_outside_rect)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, total_steps=TRAINING_STEPS)\n",
    "model.train()\n",
    "# running_loss = 0.0\n",
    "for i, (subvolumes, inklabels) in tqdm(enumerate(train_loader), total=TRAINING_STEPS):\n",
    "    if i >= TRAINING_STEPS:\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(subvolumes.to(DEVICE))\n",
    "    loss = criterion(outputs, inklabels.to(DEVICE))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "#     running_loss += loss.item()\n",
    "#     if i % 3000 == 3000-1:\n",
    "#         print(\"Loss:\", running_loss / 3000)\n",
    "#         running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll generate a prediction image. We'll use the model to predict the presence of ink for each pixel in our rectangle (the val set).\n",
    "\n",
    "This should take about a minute.\n",
    "\n",
    "Remember that the model has never seen the label data within the rectangle before!\n",
    "\n",
    "We'll plot it side-by-side with the label image. Are you able to recognize the letter \"P\" in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = SubvolumeDataset(image_stack, label, pixels_inside_rect)\n",
    "eval_loader = data.DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "output = torch.zeros_like(label).float()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (subvolumes, _) in enumerate(tqdm(eval_loader)):\n",
    "        for j, value in enumerate(model(subvolumes.to(DEVICE))):\n",
    "            output[tuple(pixels_inside_rect[i*BATCH_SIZE+j])] = value\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(output.cpu(), cmap='gray')\n",
    "ax2.imshow(label.cpu(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our output has to be binary, we have to choose a threshold, say 40% confidence.\n",
    "\n",
    "THRESHOLD = 0.4\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(output.gt(THRESHOLD).cpu(), cmap='gray')\n",
    "ax2.imshow(label.cpu(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, Kaggle expects a runlength-encoded submission.csv file, so let's output that.\n",
    "\n",
    "# Adapted from https://www.kaggle.com/code/stainsby/fast-tested-rle/notebook\n",
    "# and https://www.kaggle.com/code/kotaiizuka/faster-rle/notebook\n",
    "def rle(output):\n",
    "    pixels = np.where(output.flatten().cpu() > THRESHOLD, 1, 0).astype(np.uint8)\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "rle_output = rle(output)\n",
    "# This doesn't make too much sense, but let's just output in the required format\n",
    "# so notebook works as a submission. :-)\n",
    "print(\"Id,Predicted\\na,\" + rle_output + \"\\nb,\" + rle_output, file=open('submission.csv', 'w'))\n",
    "\n",
    "# Hurray! We've detected ink! Now, can you do better? :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuviusgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
